{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820bc0f1-8e0b-4856-82df-307f40a8c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/rosireddyvangala/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243d968",
   "metadata": {},
   "source": [
    "## Downloading the model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562887d7-e93e-48d4-a28f-55a502f28197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/misenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a993d3fbae84683bf7939cf28a9ccfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5814d8e3a176410cbdbd8b9d25874480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:  75%|#######4  | 3.39G/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de1b15de5394eec95fc6483440f52c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  39%|###9      | 3.89G/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451118f76c224a299f4cfa979595312e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:  60%|######    | 3.06G/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d823de59b5004e63b3a8e8b4ebe9c494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:  30%|##9       | 2.97G/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/ea/00/ea00943d992c7851ad9f4f4bd094a0397fb5087e0f7cba4ef003018963ea07e3/2f237251ac3ecb3bcbd8978b3eb7b55b5e83c06cd5224b276d2d8462773488c8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00002.bin%3B+filename%3D%22pytorch_model-00001-of-00002.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1722242928&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjI0MjkyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lYS8wMC9lYTAwOTQzZDk5MmM3ODUxYWQ5ZjRmNGJkMDk0YTAzOTdmYjUwODdlMGY3Y2JhNGVmMDAzMDE4OTYzZWEwN2UzLzJmMjM3MjUxYWMzZWNiM2JjYmQ4OTc4YjNlYjdiNTViNWU4M2MwNmNkNTIyNGIyNzZkMmQ4NDYyNzczNDg4Yzg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=RsTA7uOwZShwywX3GOOJPng2ZKeHXtqgVpVT%7E8pH2JvIxz0dTB2pWQRX360ONOGk5WxHhP6gjz3S71pSY7TXGoEHOt%7EP8vXth8dTjmB0mI8WOdJUSAypPHKOlY0rEOXybrUxEVuIUHukjtEpp39Ds9SJ2DGHYFERrWqCiugquw3pDs6ygXEIBdzwSimFrQEFwjFWlrbWZcbUhIMF5EsQ2Y7LJ8YRPwqI0DkrDyJh12DvguFGAX2mbZ8pjwd5OppvwLvO84rF6-VQ2ZNo1Q3oSB2192qx%7Eey2hqRDpZULqEHZv0sdG--tkd-fkTaUdymucj0vVkBmH5G-yADBWfO8-g__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63eb8fbfeea24dc79f8c1b9509958b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:  90%|######### | 8.95G/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/rosireddyvangala/Documents/Mistral_7b/model'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "local_dir = \"./model\"\n",
    "\n",
    "snapshot_download(repo_id=model_name, local_dir=local_dir, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7bb711",
   "metadata": {},
   "source": [
    "## Importing the requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77c49ba-562b-4aac-83b8-6299e53211c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pypdf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d464f59",
   "metadata": {},
   "source": [
    "## Loading the model from our local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dc4eee-a832-496a-863d-21cead42a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/misenv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1405: UserWarning: Current model requires 520097664 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d60b249632498fa6941a4fae142a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48600d0e",
   "metadata": {},
   "source": [
    "## Function to extract the data from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aff243-720c-4a0d-bc17-d7ce05ded7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in tqdm(reader.pages, desc=\"Extracting PDF\"):\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9116df3",
   "metadata": {},
   "source": [
    "## Function to generate the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1ada85-30c7-4805-8866-46eb319b864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context, num_questions=2):\n",
    "    prompt = f\"Based on the following text, generate {num_questions} relevant questions:\\n\\n{context}\\n\\nQuestions:\\n1.\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=1024,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    questions = generated_text.split(\"Questions:\")[1].strip().split(\"\\n\")\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71b257",
   "metadata": {},
   "source": [
    "## Loading and extracting the data from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93185cba-fefd-48d9-8217-37f80896c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting PDF: 100%|████| 62/62 [00:00<00:00, 103.90it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./budget_speech.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8021c",
   "metadata": {},
   "source": [
    "## Defining the Chunk size and dividing the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91437049-3ab1-47a2-aab2-7eb385f5acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 1000\n",
    "chunks = [pdf_text[i:i+chunk_size] for i in range(0, len(pdf_text), chunk_size)]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85fe3cd-904e-4c0b-ab2e-f41510fd2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_chunks=chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf95e5",
   "metadata": {},
   "source": [
    "## Generating the questions based on the data from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8332806-7bf4-441b-9e4a-e8a4a3388427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   0%|         | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating questions:  50%|▌| 1/2 [36:19<36:19, 2179.07s/iSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating questions: 100%|█| 2/2 [8:10:50<00:00, 14725.07\n"
     ]
    }
   ],
   "source": [
    "all_questions = []\n",
    "for chunk in tqdm(less_chunks, desc=\"Generating questions\"):\n",
    "    questions = generate_questions(chunk)\n",
    "    all_questions.extend(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d6e0d1-9a2d-4c7c-ba84-c73dc1fc023b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the budget theme for the financial year 2024-2025?',\n",
       " '2. What are the main priorities of the government in terms of human resource development and social justice for the financial year 2024-2025?',\n",
       " '1. What is the historic third term under the leadership of the government being referred to?',\n",
       " '2. What are the significant downside risks for growth and upside risks to inflation in the global economy?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c328941-5bb1-46cc-94b4-40017c7ca116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1. What is the budget theme for the financial year 2024-2025?\n",
      "2. 2. What are the main priorities of the government in terms of human resource development and social justice for the financial year 2024-2025?\n",
      "3. 1. What is the historic third term under the leadership of the government being referred to?\n",
      "4. 2. What are the significant downside risks for growth and upside risks to inflation in the global economy?\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(all_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ee3824-769f-4497-857e-fc47d69493ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991f50caea04431f94c0875af0d55f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk.\n",
      "Extracting PDF: 100%|████| 62/62 [00:00<00:00, 134.85it/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or 'quit' to exit):  What are the main priorities of the government in terms of human resource development and social justice for the financial year 2024-2025?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The government has identified the following priorities in terms of human resource development and social justice for the financial year 2024-2025:\n",
      "\n",
      "(iii) Inclusive Human Resource Development and Social Justice\n",
      "(iv) Employment & Skilling\n",
      "\n",
      "These priorities are mentioned in Part A of the Budget Estimates 2024-25, under the section \"Budget Priorities\".\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pypdf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in tqdm(reader.pages, desc=\"Extracting PDF\"):\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Load the Mistral-7B-Instruct-v0.1 model and tokenizer\n",
    "model_path = \"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Function to generate answer\n",
    "def generate_answer(context, question):\n",
    "    prompt = f\"\"\"<s>[INST] Given the following context, answer the question:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# Main execution\n",
    "pdf_path = \"./budget_speech.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# You might need to chunk the PDF text if it's too long\n",
    "max_context_length = 2048  # Adjust based on your GPU memory\n",
    "context_chunks = [pdf_text[i:i+max_context_length] for i in range(0, len(pdf_text), max_context_length)]\n",
    "required_chunk=context_chunks[:1]\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter your question (or 'quit' to exit): \")\n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    answer = generate_answer(required_chunk, question)\n",
    "    if answer and not answer.lower().startswith(\"i'm sorry\") and not answer.lower().startswith(\"i apologize\"):\n",
    "        print(\"Answer:\", answer)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Sorry, I couldn't find a relevant answer in the provided context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f18e331-c002-4ead-9347-28b95f2606e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(context_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d9203-4ae7-4811-b6e9-cd794c34d61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
